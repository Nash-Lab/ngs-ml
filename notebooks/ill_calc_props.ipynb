{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10758773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "def min_max_norm(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "#User inputs; NOTE: First element in arrays will match with the lowest bin number etc.\n",
    "\n",
    "input_file = 'Bins.tsv'\n",
    "output_file = 'p_Bins.tsv'\n",
    "\n",
    "delim = '\\t'\n",
    "\n",
    "aa_column  = 'aa_mutation' # Mutation column name \n",
    "n_aa_column = 'n_aa_substitutions' # Number of mutations column name\n",
    "bin_column = 'bin' # Bin number column name\n",
    "bc_column = 'barcode' # Barcode string column name\n",
    "\n",
    "expr_bins = [1,2,3,4] # Bins with expression data\n",
    "va_ref_bin = [5] # Reference bin for viscosity agent\n",
    "va_bins = [6,8,10,12,14] # Positive bins for viscosity agent experiment\n",
    "va_cons = [.0,.05,.1,.15,.2] # viscosity agent concentrations\n",
    "\n",
    "wild_type = 'Z1000Z ' #Create an out of range variant for the wild-type\n",
    "\n",
    "expr_scs  = min_max_norm(np.array([489,2902,8999,17745])) + 1 # Median expression in expression experiment\n",
    "\n",
    "count_cells = np.array([16709793,3754589,2834337,2249482,2000000,187000,2000000,230000,1951798,340426,2018790,\n",
    "                        506977,1582778,969420]) # number of total measured cells per bin\n",
    "total_reads = np.array([32964833,30377459,25372847,29016725,9809899,16868280,24891324,22768756,14914290,10776556,\n",
    "                        15074880,16724952,17855672,23342164]) # total number of reads per bin\n",
    "\n",
    "amp_factors = count_cells / total_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_file, delimiter=delim)\n",
    "\n",
    "#cf = df.copy().fillna(wild_type)\n",
    "cf = df.copy().replace(to_replace=r'^ $', value=wild_type, regex=True)\n",
    "\n",
    "min_bin = cf[bin_column].min()\n",
    "\n",
    "min_bin\n",
    "\n",
    "cf['c_size'] = cf['size'].astype(float)\n",
    "\n",
    "for i in cf.index:\n",
    "    cf.at[i,'c_size'] = cf.at[i,'size'] * amp_factors[int(cf.at[i,bin_column] - min_bin)]\n",
    "    \n",
    "cf = cf.rename(columns={'size':'i_size'})\n",
    "\n",
    "\n",
    "for bi in np.arange(-1,len(va_bins)):\n",
    "    if bi == -1:\n",
    "        tf = cf.query(f\"{bin_column} in @expr_bins\")\n",
    "    else:\n",
    "        curr_bin = np.atleast_1d(va_bins[bi])\n",
    "        tf = cf.query(f\"{bin_column} in @va_ref_bin or {bin_column} in @curr_bin\")\n",
    "\n",
    "    tm = tf[[aa_column,bc_column]].drop_duplicates().groupby(\n",
    "        [aa_column], as_index=False\n",
    "    ).size().rename(columns={'size':'n_barcodes'})\n",
    "    tf = tf.merge(tm, on=[aa_column], how='outer')\n",
    "\n",
    "    tm = tf[[aa_column,bin_column,'c_size']].groupby([aa_column,bin_column], as_index=False).sum()\n",
    "    tf = tf.merge(tm, on=[aa_column,bin_column], how='outer')\n",
    "\n",
    "    tf = tf.rename(columns={'c_size_x':'c_size','c_size_y':'cm_size'})\n",
    "\n",
    "    tm = tf[[aa_column,'cm_size']].drop_duplicates().groupby([aa_column], as_index=False).sum()\n",
    "    tf = tf.merge(tm, on=[aa_column], how='outer')\n",
    "\n",
    "    tf = tf.rename(columns={'cm_size_x':'cm_size','cm_size_y':'cmt_size'})\n",
    "\n",
    "    tm = tf[[bc_column,'c_size']].groupby([bc_column], as_index=False).sum()\n",
    "    tf = tf.merge(tm, on=[bc_column], how='outer')\n",
    "\n",
    "    tf = tf.rename(columns={'c_size_x':'c_size','c_size_y':'cb_size'})\n",
    "\n",
    "    #Explained variation\n",
    "\n",
    "    wm = lambda x: np.average(x, weights=rf.loc[x.index,'c_size'])\n",
    "\n",
    "    rf = tf.assign(b_score=lambda x: x['c_size'] / x['cb_size']).copy()\n",
    "\n",
    "    tm = rf[[aa_column,bin_column,'b_score']].groupby(\n",
    "        [aa_column, bin_column], as_index=False\n",
    "    ).agg(bb_score=('b_score',wm))\n",
    "    rf = rf.merge(tm, on=[aa_column, bin_column], how='outer')\n",
    "\n",
    "    rf = rf.assign(diff=lambda x: x['b_score'] - x['bb_score'])\n",
    "\n",
    "    tm = rf[[aa_column,'diff','b_score']].groupby([aa_column]).agg(diff_avg=('diff',wm),true_avg=('b_score',wm))\n",
    "    rf = rf.merge(tm, on=[aa_column], how='outer')\n",
    "\n",
    "    rf = rf.assign(nom=lambda x: (x['b_score'] - x['bb_score'] - x['diff_avg'])**2)\n",
    "    rf = rf.assign(denom=lambda x: (x['b_score'] - x['true_avg'])**2)\n",
    "\n",
    "    tm = rf[[aa_column,'nom','denom']].groupby([aa_column]).agg(nom_avg=('nom',wm),denom_avg=('denom',wm))\n",
    "    rf = rf.merge(tm, on=[aa_column], how='outer')\n",
    "\n",
    "    rf['evs'] = rf.apply(lambda x: 1 if x['denom_avg'] == 0 else 1 - x['nom_avg'] / x['denom_avg'], axis=1)\n",
    "\n",
    "    rf = rf.assign(\n",
    "        conf=lambda x: (1-(1/1.5**np.clip(x['n_barcodes'],0,32))) * x['cmt_size'] * x['evs']\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    an = rf[[aa_column,n_aa_column,'n_barcodes','cmt_size','conf','evs']].drop_duplicates()\\\n",
    "    .sort_values(by=['conf','cmt_size','n_barcodes','evs'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if bi == -1:\n",
    "        antm = an.rename(\n",
    "            columns={'n_barcodes':'exp_n_barcodes', 'cmt_size':'exp_cmt_size', 'conf':'exp_conf', 'evs':'exp_evs'}\n",
    "        ).copy()\n",
    "    else:\n",
    "        an = an.rename(columns={'n_barcodes':f'{va_cons[bi]:.2f}_n_barcodes',\n",
    "                                'cmt_size':f'{va_cons[bi]:.2f}_cmt_size',\n",
    "                                'conf':f'{va_cons[bi]:.2f}_conf',\n",
    "                                'evs':f'{va_cons[bi]:.2f}_evs'}).copy()\n",
    "        \n",
    "        antm = antm.merge(an, on=[aa_column, n_aa_column], how='outer')\n",
    "\n",
    "m_cf = cf.copy().groupby([aa_column,n_aa_column,bin_column], as_index=False).sum()\n",
    "\n",
    "ff = m_cf.query(f\"{bin_column} in @expr_bins\").copy()\n",
    "\n",
    "for i in ff.index:\n",
    "    ff.at[i,'weight'] = expr_scs[ff.at[i,bin_column]-1]\n",
    "\n",
    "tm = ff[[aa_column,'c_size']].groupby([aa_column], as_index=False).sum()\n",
    "ff = ff.merge(tm, on=[aa_column], how='outer')\n",
    "\n",
    "ff = ff.assign(b_score=lambda x: x['weight'] * x['c_size_x'] / x['c_size_y'])\n",
    "\n",
    "tm = ff[[aa_column,'b_score']].groupby([aa_column], as_index=False).sum()\n",
    "ff = ff.merge(tm, on=[aa_column], how='outer')\n",
    "\n",
    "wt_expr = ff.query(f\"{aa_column} == '{wild_type}'\")['b_score_y'].mean()\n",
    "\n",
    "for i in ff.index:\n",
    "\n",
    "    ff.at[i,'expr'] = np.log2( ff.at[i,'b_score_y'] / wt_expr ) if ff.at[i,'b_score_y'] != 0 else np.nan\n",
    "\n",
    "ef = ff[[aa_column,'expr']].drop_duplicates()\n",
    "\n",
    "full = antm.merge(ef, on=[aa_column], how='outer').copy()\n",
    "\n",
    "for bi in np.arange(len(va_bins)):\n",
    "    curr_bin = np.atleast_1d(va_bins[bi])\n",
    "    ff = m_cf.query(\n",
    "        f\"{bin_column} in @va_ref_bin or {bin_column} in @curr_bin\"\n",
    "    ).copy().sort_values(by=[aa_column,bin_column])\n",
    "    \n",
    "    j=-1\n",
    "    this_mut=''\n",
    "    \n",
    "    for i in ff.index:\n",
    "        \n",
    "        if ff.at[i,bin_column] in va_ref_bin:\n",
    "            j=i\n",
    "            this_mut = ff.at[i,aa_column]\n",
    "        elif ff.at[i,bin_column] == va_bins[bi] and this_mut == ff.at[i,aa_column]:\n",
    "            ff.at[i,'ratio'] = ff.at[i,'i_size'] / ff.at[j,'i_size']\n",
    "            \n",
    "    \n",
    "    wt_enri = ff.query(f\"{aa_column} == '{wild_type}'\")['ratio'].sum()\n",
    "    ff[f'{va_cons[bi]:.2f}_enrich'] = np.log2(ff['ratio'] / wt_enri)\n",
    "    fftm = ff[[aa_column,f'{va_cons[bi]:.2f}_enrich']].dropna()\n",
    "    full = full.merge(fftm, on=[aa_column], how='outer')\n",
    "    \n",
    "#full.sort_values(by=[aa_column]).reset_index(drop=True).to_csv(output_file,index=False)\n",
    "\n",
    "af = full.copy()\n",
    "\n",
    "for i in af.index:\n",
    "    \n",
    "    syn = []\n",
    "    \n",
    "    #arr = af.at[i,aa_column].findall(r'[A-Z*]+')\n",
    "    arr = re.findall(r'[A-Z*]+', af.at[i,aa_column])\n",
    "    \n",
    "    if '*' in arr:\n",
    "        af.at[i,'tag'] = 1\n",
    "    else:\n",
    "        af.at[i,'tag'] = 0\n",
    "    \n",
    "    for j in np.arange(0,len(arr),2):\n",
    "        if arr[j] == arr[j+1] and arr[j] != 'Z':\n",
    "            syn.append(1)\n",
    "        else:\n",
    "            syn.append(0)\n",
    "    \n",
    "    if np.sum(syn) == len(syn):\n",
    "        af.at[i,'tag'] = 2\n",
    "            \n",
    "    \n",
    "    aam = []\n",
    "    \n",
    "    for j,s in enumerate(af.at[i,'aa_mutation'].split()):\n",
    "        if syn[j] == 0:\n",
    "            aam.append(s)\n",
    "            \n",
    "    if len(aam) == 0:\n",
    "        aam.append('Z1000Z')\n",
    "        syn.append(-1)\n",
    "        \n",
    "    fs = \"\"\n",
    "    for e in aam:\n",
    "        fs += e+\" \"\n",
    "    \n",
    "    af.at[i,'aa_mutation_syn'] = fs\n",
    "    af.at[i,'n_aa_substitutions_syn'] = af.at[i,'n_aa_substitutions'] - np.sum(syn)\n",
    "    \n",
    "af.sort_values(by=[aa_column]).reset_index(drop=True).to_csv(output_file,index=False)\n",
    "af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819d7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "peg = '00'\n",
    "g = sns.histplot(data=af.rename(columns={f'0.{peg}_evs':'evs', f'0.{peg}_cmt_size':'cmt_size', f'0.{peg}_conf':'conf'}).query(\" evs != 1.0 and n_aa_substitutions_syn <= 1 and conf > 10\"), x=f'0.{peg}_enrich', hue='tag', palette='tab10', )#multiple='stack')\n",
    "g.set(xlim=(-10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(data=af.query(\" exp_evs != 1.0 and exp_cmt_size > 1 and n_aa_substitutions_syn <= 1\"), x='expr', hue='tag', palette='tab10', )#multiple='stack')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:eeopt]",
   "language": "python",
   "name": "conda-env-eeopt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
